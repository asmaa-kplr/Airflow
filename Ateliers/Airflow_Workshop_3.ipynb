{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Construire un pipeline d'exécution avec PostgreSQL"
      ],
      "metadata": {
        "id": "JG8TvnNJ8lNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration initiale :\n",
        "\n",
        "Il est recommandé d'avoir Docker installé, car la procédure \"Exécution d'Airflow dans Docker\" sera utilisée pour cet exemple."
      ],
      "metadata": {
        "id": "6W8mGwRj8o-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHb8wqYM75j0"
      },
      "outputs": [],
      "source": [
        "#Télécharger le fichier docker-compose.yaml\n",
        "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veuillez apporter les modifications suivantes dans le fichier docker-compose.yaml :\n",
        "* (Ligne 53) :      \n",
        "   "
      ],
      "metadata": {
        "id": "u6jfvOYs8x8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AIRFLOW__CORE__EXECUTOR: LocalExecutor"
      ],
      "metadata": {
        "id": "56kyZkiq9u5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Commentez les lignes (57-58) :"
      ],
      "metadata": {
        "id": "ZpEExyul9uFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n",
        "# AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0"
      ],
      "metadata": {
        "id": "jj-Mop1U9pmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Commentez les lignes (71-72) :"
      ],
      "metadata": {
        "id": "kCNU0Jlw9sz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # redis:\n",
        "    #   condition: service_healthy"
      ],
      "metadata": {
        "id": "_igwRUNI9_US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Commentez les lignes (91~100) :"
      ],
      "metadata": {
        "id": "0PlK02NG-Afw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # redis:\n",
        "  #   image: redis:latest\n",
        "  #   expose:\n",
        "  #     - 6379\n",
        "  #   healthcheck:\n",
        "  #     test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
        "  #     interval: 5s\n",
        "  #     timeout: 30s\n",
        "  #     retries: 50\n",
        "  #   restart: always"
      ],
      "metadata": {
        "id": "dm-xwvKb-Emr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Commentez les lignes (261~277) :"
      ],
      "metadata": {
        "id": "jVUGpGC1-TXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # flower:\n",
        "  #   <<: *airflow-common\n",
        "  #   command: celery flower\n",
        "  #   profiles:\n",
        "  #     - flower\n",
        "  #   ports:\n",
        "  #     - 5555:5555\n",
        "  #   healthcheck:\n",
        "  #     test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n",
        "  #     interval: 10s\n",
        "  #     timeout: 10s\n",
        "  #     retries: 5\n",
        "  #   restart: always\n",
        "  #   depends_on:\n",
        "  #     <<: *airflow-common-depends-on\n",
        "  #     airflow-init:\n",
        "  #       condition: service_completed_successfully"
      ],
      "metadata": {
        "id": "f6AqBJoF-T5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La configuration se poursuit comme suit :"
      ],
      "metadata": {
        "id": "Di3wkCL0-lhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Créer les répertoires attendus et définir une variable d'environnement attendue\n",
        "mkdir -p ./dags ./logs ./plugins\n",
        "echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n",
        "\n",
        "#Initialiser la base de données\n",
        "docker-compose up airflow-init\n",
        "\n",
        "#Démarrer tous les services\n",
        "docker-compose up"
      ],
      "metadata": {
        "id": "Kj7XUNLW-lEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois que tous les services ont été démarrés, l'interface utilisateur Web sera disponible à l'adresse : http://localhost:8080. Le compte par défaut a le nom d'utilisateur airflow et le mot de passe airflow.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/123757632/227974791-6c895472-4bb6-4127-a694-d105f2524b2f.png)"
      ],
      "metadata": {
        "id": "wuafbKhV-7mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration PosgreSQL via l'interface web\n",
        "\n",
        "Il sera également nécessaire de créer une connexion à la base de données postgres. Pour effectuer cette opération via l'interface Web, accédez au menu \"Admin\", sélectionnez \"Connections\", puis cliquez sur l'icône \"+\" pour ajouter un nouvel enregistrement à la liste des connexions.\n",
        "\n",
        "Les champs doivent être remplis selon les indications ci-dessous. Il est important de noter l'identifiant de connexion (Connection Id), qui sera passé en paramètre pour l'argument nommé postgres_conn_id."
      ],
      "metadata": {
        "id": "VlKPwvrm_cb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tâches de création de table\n",
        "\n",
        "Il est possible d'utiliser PostgresOperator afin de définir des tâches qui créeront des tables dans la base de données postgres.\n",
        "\n",
        "Il est prévu de créer deux tables : une pour faciliter le nettoyage des données (employees_temp) et une autre pour stocker les données nettoyées (employees).\n",
        "\n",
        "* Tâche 1 :  \n",
        "  - Tâche nommée \"create_employees_table\". \n",
        "  - Cette tâche est associée à une instance de PostgreSQL. \n",
        "  - La tâche exécute la commande SQL pour créer une table nommée \"employees\"  si elle n'existe pas avec des colonnes : \n",
        "      * \"Serial Number\" NUMERIC PRIMARY KEY,\n",
        "      * \"Company Name\" TEXT,\n",
        "      * \"Employee Markme\" TEXT,\n",
        "      * \"Description\" TEXT,\n",
        "      * \"Leave\" INTEGER\n",
        "\n",
        "* Tâche 2 : \n",
        "  - Tâche nommée \"create_employees_temp_table\".\n",
        "  - Cette tâche est associée à une instance de PostgreSQL. \n",
        "  - La tâche exécute une commande SQL pour supprimer une table nommée \"employees_temp\" si elle existe, puis crée une nouvelle table portant le même nom avec des colonnes : \n",
        "      * \"Serial Number\" NUMERIC PRIMARY KEY,\n",
        "      * \"Company Name\" TEXT,\n",
        "      * \"Employee Markme\" TEXT,\n",
        "      * \"Description\" TEXT,\n",
        "      * \"Leave\" INTEGER"
      ],
      "metadata": {
        "id": "5MLbZoR1VW9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tâche de récupération de données"
      ],
      "metadata": {
        "id": "JUAMlWlgXfcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les données sont récupérées, enregistrées dans un fichier sur l'instance Airflow, puis chargées à partir de ce fichier dans une table intermédiaire afin de pouvoir exécuter les étapes de nettoyage des données.\n",
        "\n"
      ],
      "metadata": {
        "id": "zteFhACYXt3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définir une fonction nommée **\"get_data()\"** qui est décorée par **\"@task\"**, indiquant qu'il s'agit d'une tâche Airflow. \n",
        "\n",
        "* Cette tâche télécharge un fichier CSV à partir de l'URL \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/tutorial/pipeline_example.csv\" , \n",
        "\n",
        "* Utilise la bibliothèque **\"requests\"** pour envoyer une requête GET à l'URL et récupérer les données. \n"
      ],
      "metadata": {
        "id": "xZe90fYKYiVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/tutorial/pipeline_example.csv\""
      ],
      "metadata": {
        "id": "LlGhL2Goa1Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Les données sont ensuite écrites dans un fichier situé dans le chemin \"data_path\", qui est défini pour être \"/opt/airflow/dags/files/employees.csv\". Si le chemin n'existe pas, il est créé .  \n"
      ],
      "metadata": {
        "id": "yifYBLwhbAw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/opt/airflow/dags/files/employees.csv\"\n",
        "os.makedirs(os.path.dirname(data_path), exist_ok=True)"
      ],
      "metadata": {
        "id": "Q9peiW-Fa_um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Ensuite, la tâche utilise la bibliothèque **\"psycopg2\"** pour se connecter à la base de données PostgreSQL avec l'identifiant de connexion. \n"
      ],
      "metadata": {
        "id": "E--xT3LUbiQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "postgres_hook = PostgresHook(postgres_conn_id=\"******\")\n",
        "conn = postgres_hook.get_conn()\n",
        "cur = conn.cursor()"
      ],
      "metadata": {
        "id": "IucXUSNAbm1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Elle insère les données du fichier CSV dans une table temporaire nommée **\"employees_temp\"** à l'aide de la méthode \"copy_expert\" qui permet de copier les données du fichier dans la table.\n",
        "    \n",
        "                \n"
      ],
      "metadata": {
        "id": "ZvW1a-ehbxvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path, \"r\") as file:\n",
        "        cur.copy_expert(\n",
        "            \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n",
        "            file,\n",
        "        )"
      ],
      "metadata": {
        "id": "8H6vQgWXb0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Enfin, la tâche confirme la transaction en appelant \"conn.commit()\"."
      ],
      "metadata": {
        "id": "KAh2hJCcb0dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tâche de fusion de données\n",
        "\n",
        "Des enregistrements complètement uniques sont sélectionnés à partir des données récupérées, puis il est vérifié si des numéros de série d'employés sont déjà présents dans la base de données. Si tel est le cas, ces enregistrements sont mis à jour avec les nouvelles données."
      ],
      "metadata": {
        "id": "1emdrENcXyU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définir une fonction nommée \"merge_data()\" qui est décorée par \"@task\", indiquant qu'il s'agit d'une tâche Airflow.\n",
        "\n",
        "* La tâche fusionne les données d'une table temporaire appelée **employees_temp** dans une table permanente appelée **employees**. Pour ce faire, elle sélectionne toutes les lignes distinctes de employees_temp et les insère dans employees, ou met à jour les lignes existantes s'il y a un conflit sur la colonne \"Serial Number\".\n",
        "\n"
      ],
      "metadata": {
        "id": "Xki51jruYi2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La tâche est exécutée à l'aide d'un **PostgresHook**, qui se connecte à une base de données **PostgreSQL** et exécute des requêtes SQL. \n",
        "\n",
        "* La requête SQL utilisée dans cette tâche est définie dans la variable de requête et utilise la syntaxe INSERT INTO ... "
      ],
      "metadata": {
        "id": "-n35qTYic0ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "        INSERT INTO employees\n",
        "        SELECT *\n",
        "        FROM (\n",
        "            SELECT DISTINCT *\n",
        "            FROM employees_temp\n",
        "        ) t\n",
        "        ON CONFLICT (\"Serial Number\") DO UPDATE\n",
        "        SET\n",
        "              \"Employee Markme\" = excluded.\"Employee Markme\",\n",
        "              \"Description\" = excluded.\"Description\",\n",
        "              \"Leave\" = excluded.\"Leave\";\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "58IJcF-kcuCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ON CONFLICT de Postgres pour gérer les conflits lors de l'insertion de données dans la table employees.\n",
        "Si la tâche s'exécute avec succès, elle renvoie 0. Si une exception se produit, elle renvoie 1. (Utilisez Exception)"
      ],
      "metadata": {
        "id": "c83faHtudG_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalisation du DAG"
      ],
      "metadata": {
        "id": "CJTu28AyYTK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Utiliser l'opérateur **>>** pour définir les dépendances entre les tâches. Cela indique que les tâches à gauche de l'opérateur doivent être exécutées avant la tâche à droite de l'opérateur.\n",
        "\n",
        "* Plus précisément, la ligne de code établit les dépendances suivantes :\n",
        "\n",
        "  * Les tâches create_employees_table et create_employees_temp_table doivent être exécutées avant la tâche get_data().\n",
        "  * La tâche get_data() doit être exécutée avant la tâche merge_data()."
      ],
      "metadata": {
        "id": "tDoevjg2YjgY"
      }
    }
  ]
}